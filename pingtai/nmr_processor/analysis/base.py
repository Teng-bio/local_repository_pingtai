#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
åŸºç¡€åˆ†ææ¨¡å— - æä¾›NMRæ•°æ®åŸºç¡€åˆ†æåŠŸèƒ½
ğŸ”§ ä¿®å¤ç‰ˆæœ¬ï¼šæ­£ç¡®å¤„ç†å³°æ•°æ®ç±»å‹
"""

import os
import logging
import time
import pandas as pd
import json
from pathlib import Path
from ..config import Config
from ..utils.decorators import memory_monitor
from ..data.loader import load_nmr_data, load_gcf_matrix

@memory_monitor
def analyze_nmr_data(gcf_matrix_path=None, nmr_data_path=None, output_path=None):
    """åŸºç¡€NMRæ•°æ®åˆ†æåŠŸèƒ½
    
    å¯¹æ¯ä¸ªèŒæ ªçš„NMRæ•°æ®è¿›è¡ŒåŸºæœ¬åˆ†æï¼Œç”Ÿæˆç®€å•æŠ¥å‘Šã€‚
    
    å‚æ•°:
        gcf_matrix_path (str): GCFçŸ©é˜µæ–‡ä»¶è·¯å¾„
        nmr_data_path (str): NMRæ•°æ®ç›®å½•
        output_path (str): è¾“å‡ºç›®å½•
    
    è¿”å›:
        str: è¾“å‡ºç›®å½•è·¯å¾„
    """
    # ä½¿ç”¨é…ç½®å€¼ä½œä¸ºé»˜è®¤å€¼
    if output_path is None:
        output_path = Config.get_output_path()
    if gcf_matrix_path is None:
        gcf_matrix_path = Config.get_gcf_matrix_path()
    if nmr_data_path is None:
        nmr_data_path = Config.get_nmr_data_path()
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_path, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    logging.info("åŠ è½½GCFçŸ©é˜µæ•°æ®...")
    gcf_data = load_gcf_matrix(gcf_matrix_path)
    
    logging.info("åŠ è½½NMRå³°æ•°æ®...")
    peak_data = load_nmr_data(nmr_data_path)
    
    # ğŸ”§ è½¬æ¢ä¸ºèŒæ ªçº§åˆ«çš„æ˜ å°„ - æ­£ç¡®å¤„ç†å­—å…¸æ ¼å¼
    strain_to_peaks = {}
    for sample_id, peak_info in peak_data.items():
        # peak_info æ˜¯å­—å…¸: {'peaks': [...], 'fraction': 'Fr1', 'strain': 'QT1'}
        if isinstance(peak_info, dict):
            strain_id = peak_info.get('strain')
            if not strain_id:
                # é™çº§å¤„ç†ï¼šä»sample_idæå–
                strain_id = sample_id.split('_')[0]
        else:
            # å…¼å®¹æ—§æ ¼å¼
            strain_id = sample_id.split('_')[0]
        
        if strain_id not in strain_to_peaks:
            strain_to_peaks[strain_id] = []
        
        # ä¿å­˜å®Œæ•´çš„peak_infoï¼ˆåŒ…å«fractionç­‰ä¿¡æ¯ï¼‰
        strain_to_peaks[strain_id].append(peak_info)
    
    # åŸºæœ¬åˆ†æ
    results = {}
    for strain_id, peaklists in strain_to_peaks.items():
        # åˆå¹¶æ‰€æœ‰æ ·æœ¬çš„å³° - ğŸ”§ ç¡®ä¿æ‰€æœ‰å³°å€¼éƒ½æ˜¯æ•°å€¼ç±»å‹
        all_peaks = []
        for peak_info in peaklists:
            if isinstance(peak_info, dict):
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼ˆåŒ…å«peaks, fractionç­‰ï¼‰
                peaks = peak_info.get('peaks', [])
            else:
                # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼
                peaks = peak_info
            
            # ğŸ”§ è¿‡æ»¤å¹¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            for peak in peaks:
                try:
                    peak_float = float(peak)
                    if 0 <= peak_float <= 15:  # åˆç†èŒƒå›´æ£€æŸ¥
                        all_peaks.append(peak_float)
                except (ValueError, TypeError):
                    logging.debug(f"è·³è¿‡æ— æ•ˆå³°å€¼: {peak}")
                    continue
        
        # è®¡ç®—ç®€å•ç»Ÿè®¡
        min_val = min(all_peaks) if all_peaks else None
        max_val = max(all_peaks) if all_peaks else None
        avg_val = sum(all_peaks) / len(all_peaks) if all_peaks else None
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¯¹åº”çš„GCFæ•°æ®
        has_gcf = strain_id in gcf_data
        gcf_count = len(gcf_data.get(strain_id, [])) if has_gcf else 0
        
        results[strain_id] = {
            'peak_count': len(all_peaks),
            'sample_count': len(peaklists),
            'min_ppm': min_val,
            'max_ppm': max_val,
            'avg_ppm': avg_val,
            'has_gcf': has_gcf,
            'gcf_count': gcf_count
        }
        
        # ä¿å­˜å•ä¸ªèŒæ ªçš„å³°æ•°æ®
        strain_output_file = os.path.join(output_path, f"{strain_id}_peaks.csv")
        pd.DataFrame({'ppm': all_peaks}).to_csv(strain_output_file, index=False)
        logging.info(f"ä¿å­˜èŒæ ª {strain_id} çš„å³°æ•°æ®åˆ° {strain_output_file}")
    
    # ä¿å­˜æ€»ä½“åˆ†æç»“æœ
    summary_file = os.path.join(output_path, "analysis_summary.json")
    with open(summary_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    logging.info(f"åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åˆ° {output_path}")
    return output_path
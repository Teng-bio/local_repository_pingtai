#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ•°æ®åŠ è½½æ¨¡å— - ç»Ÿä¸€å¤„ç†NMRå’ŒGCFæ•°æ®çš„åŠ è½½åŠŸèƒ½
ğŸ”§ ä¿®å¤ç‰ˆæœ¬ï¼šæ­£ç¡®æå–fractionä¿¡æ¯å’Œå¤„ç†æ•°æ®ç±»å‹
"""

import os
import pandas as pd
import logging
import chardet
import re
from pathlib import Path
from collections import defaultdict
from sklearn.cluster import DBSCAN
import numpy as np
from ..utils.decorators import memory_monitor
from ..utils.memory_protector import (
    MemoryProtector, 
    with_memory_protection,
    memory_limit,
    MemoryLeakDetector,
    emergency_cleanup
)
from ..config import Config
from ..analysis.peak_classifier import PeakClassifierFilter

# è®¾ç½®æ—¥å¿—è®°å½•
def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    try:
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(Config.get_log_path()), exist_ok=True)
        
        # ç„¶åé…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s',
            handlers=[
                logging.FileHandler(Config.get_log_path()),
                logging.StreamHandler()
            ]
        )
    except Exception as e:
        # å‡ºé”™æ—¶å›é€€åˆ°ä»…æ§åˆ¶å°æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s [%(levelname)s] %(message)s'
        )
        logging.warning(f"è®¾ç½®æ–‡ä»¶æ—¥å¿—å¤±è´¥: {e}")


# ğŸ”§ ä¿®å¤ï¼šä»sample_idæå–fractionä¿¡æ¯ - æ­£ç¡®å¤„ç†å„ç§æ ¼å¼
def extract_fraction_info(sample_id):
    """
    ä»sample_idä¸­æå–fractionä¿¡æ¯
    
    æ”¯æŒçš„æ ¼å¼ï¼š
    - QT1-Fr1, QT1_Fr1
    - SDU37-Fr2, SDU37_fr2
    - 09-6S3-Fr3
    - QT1_fr2_peaks (æ–‡ä»¶åæ ¼å¼)
    - /path/003C31/003C31-Fr1/10 (è·¯å¾„æ ¼å¼)
    
    å‚æ•°:
        sample_id (str): æ ·æœ¬IDï¼ˆå¯èƒ½æ¥è‡ªæ–‡ä»¶åæˆ–è·¯å¾„ï¼‰
        
    è¿”å›:
        tuple: (strain_id, fraction)ï¼Œå¦‚æœæ— æ³•æå–è¿”å›(sample_id, 'unknown')
    """
    # ğŸ”§ æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼ï¼šåŒ¹é… -Fr[1-5] æˆ– _fr[1-5]ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰ï¼Œå¯ä»¥ä¸åœ¨æœ«å°¾
    # ä½¿ç”¨(?:_peaks)?$ æ¥å¤„ç† _peaks åç¼€ï¼ˆå¯é€‰ï¼‰
    fraction_pattern = r'[-_](Fr[1-5])(?:[-_]peaks)?(?:$|/)'
    match = re.search(fraction_pattern, sample_id, re.IGNORECASE)
    
    if match:
        fraction = match.group(1).capitalize()  # ç»Ÿä¸€ä¸º Fr1, Fr2 ç­‰æ ¼å¼
        # æå–strainéƒ¨åˆ†ï¼ˆfractionä¹‹å‰çš„æ‰€æœ‰å†…å®¹ï¼‰
        strain = sample_id[:match.start()]
        # æ¸…ç†strainä¸­çš„å°¾éƒ¨è·¯å¾„åˆ†éš”ç¬¦
        strain = strain.rstrip('/_\\')
        return strain, fraction
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ ‡å‡†æ ¼å¼ï¼Œå°è¯•ä»è·¯å¾„ä¸­æŸ¥æ‰¾
    # ä¾‹å¦‚ï¼š/home/teng/nmr/nmr_data/09-6S3/09-6S3-Fr3/10
    if '/' in sample_id or '\\' in sample_id:
        path_parts = Path(sample_id).parts
        for part in reversed(path_parts):
            match = re.search(fraction_pattern, part, re.IGNORECASE)
            if match:
                fraction = match.group(1).capitalize()
                strain = part[:match.start()].rstrip('/_\\')
                # å¦‚æœæå–çš„strainä¸ºç©ºï¼Œä½¿ç”¨å‰ä¸€ä¸ªç›®å½•å
                if not strain and len(path_parts) >= 2:
                    strain_idx = list(path_parts).index(part) - 1
                    if strain_idx >= 0:
                        strain = path_parts[strain_idx]
                return strain, fraction
    
    # å¦‚æœè¿˜æ˜¯æ— æ³•åŒ¹é…ï¼Œè¿”å›åŸæ ·
    logging.debug(f"æ— æ³•ä» '{sample_id}' æå–fractionä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼")
    return sample_id, 'unknown'


@with_memory_protection(max_memory_percent=85)
def load_gcf_matrix(matrix_path=None):
    """åŠ è½½GCFçŸ©é˜µæ–‡ä»¶
    
    å‚æ•°:
        matrix_path (str): GCFçŸ©é˜µæ–‡ä»¶çš„è·¯å¾„
            
    è¿”å›:
        dict: èŒæ ªåˆ°GCFåˆ—è¡¨çš„æ˜ å°„
    """
    # åœ¨å‡½æ•°è°ƒç”¨æ—¶è®¾ç½®æ—¥å¿—ï¼Œè€Œä¸æ˜¯åœ¨å¯¼å…¥æ—¶
    setup_logging()

    if matrix_path is None:
        matrix_path = Config.get_gcf_matrix_path()
        
    gcf_data = {}
    try:
        # æ£€æµ‹ç¼–ç 
        with open(matrix_path, 'rb') as f:
            raw = f.read(10000)
            enc = chardet.detect(raw)['encoding']
        
        with open(matrix_path, 'r', encoding=enc, errors='replace') as f:
            reader = pd.read_csv(f)
            for _, row in reader.iterrows():
                strain = row.get('Strain', '').strip()
                if not strain:
                    continue
                
                gcf_names = row.get('gcf_names', '')
                if not gcf_names:
                    logging.warning(f"èŒæ ª {strain} æ²¡æœ‰GCFæ•°æ®")
                    continue
                
                gcf_data[strain] = gcf_names.split(';')
        
        logging.info(f"å·²åŠ è½½ {len(gcf_data)} ä¸ªèŒæ ªçš„GCFæ•°æ®")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        if len(gcf_data) == 0:
            raise ValueError("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•GCFæ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
        
        return gcf_data
    except Exception as e:
        logging.error(f"å¤„ç†GCFçŸ©é˜µæ–‡ä»¶å¤±è´¥: {str(e)}")
        logging.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        raise


@with_memory_protection(max_memory_percent=85)
def load_nmr_data(peak_dir=None):
    """åŠ è½½NMRå³°æ•°æ®
    
    å‚æ•°:
        peak_dir (str): åŒ…å«å³°æ–‡ä»¶çš„ç›®å½•
            
    è¿”å›:
        dict: æ ·æœ¬IDåˆ°{peaks, fraction, strain}çš„æ˜ å°„
        æ ¼å¼: {sample_id: {'peaks': [...], 'fraction': 'Fr1', 'strain': 'QT1'}}
    """
    # åœ¨å‡½æ•°è°ƒç”¨æ—¶è®¾ç½®æ—¥å¿—ï¼Œè€Œä¸æ˜¯åœ¨å¯¼å…¥æ—¶
    setup_logging()
    
    if peak_dir is None:
        peak_dir = Config.get_nmr_data_path()
        
    peak_data = {}
    count = 0
    
    try:
        if not os.path.exists(peak_dir):
            raise FileNotFoundError(f"å³°æ•°æ®ç›®å½•ä¸å­˜åœ¨: {peak_dir}")
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
        csv_files = list(Path(peak_dir).glob("**/*_peaks.csv"))
        if not csv_files:
            csv_files = list(Path(peak_dir).glob("**/*.csv"))
        
        logging.info(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")

        peak_filter = None

        # å¦‚æœå¯ç”¨å³°åˆ†ç±»å™¨è¿‡æ»¤ï¼Œåˆå§‹åŒ–åˆ†ç±»å™¨
        if Config.USE_PEAK_CLASSIFIER:
            try:
                peak_filter = PeakClassifierFilter(
                    model_path=Config.get_peak_classifier_model_path(),
                    confidence_threshold=Config.PEAK_CONFIDENCE_THRESHOLD
                )
                logging.info("âœ“ å³°åˆ†ç±»å™¨å·²å¯ç”¨")
            except Exception as e:
                logging.warning(f"å³°åˆ†ç±»å™¨åŠ è½½å¤±è´¥ï¼Œå°†ä¸è¿›è¡Œè¿‡æ»¤: {e}")
                peak_filter = None
        
        for csv_file in csv_files:
            try:
                # ğŸ”§ ä»æ–‡ä»¶åæˆ–è·¯å¾„æå–èŒæ ªå’Œfractionä¿¡æ¯
                # ä¼˜å…ˆä»æ–‡ä»¶åæå–
                filename = csv_file.stem  # ä¸å«æ‰©å±•å
                strain_id, fraction = extract_fraction_info(filename)
                
                # å¦‚æœæ–‡ä»¶åæå–å¤±è´¥ï¼Œå°è¯•ä»è·¯å¾„æå–
                if fraction == 'unknown':
                    # ä¾‹å¦‚ï¼š/path/to/09-6S3/09-6S3-Fr3/10/nmrpipe/spectrum.csv
                    parent_dir = csv_file.parent
                    if parent_dir.name == "nmrpipe":
                        parent_dir = parent_dir.parent
                    if parent_dir.name == "10":
                        parent_dir = parent_dir.parent
                    
                    strain_id, fraction = extract_fraction_info(parent_dir.name)
                    
                    # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œå°è¯•ä»å®Œæ•´è·¯å¾„æå–
                    if fraction == 'unknown':
                        strain_id, fraction = extract_fraction_info(str(csv_file))
                
                # æ„å»ºå”¯ä¸€çš„sample_id
                sample_id = f"{strain_id}_{fraction}" if fraction != 'unknown' else strain_id
                
                # è¯»å–CSVæ–‡ä»¶
                df = pd.read_csv(csv_file)

                # === å³°åˆ†ç±»å™¨è¿‡æ»¤ ===
                if peak_filter is not None:
                    try:
                        # æ£€æŸ¥æ˜¯å¦æœ‰å¿…éœ€çš„åˆ—
                        required_cols = ['HEIGHT', 'CONFIDENCE', 'X_PPM']
                        if all(col in df.columns for col in required_cols):
                            original_count = len(df)
                            df, filter_stats = peak_filter.filter_peaks(df)
                            logging.debug(
                                f"æ–‡ä»¶ {csv_file.name}: "
                                f"è¿‡æ»¤å‰ {original_count} å³° -> "
                                f"è¿‡æ»¤å {len(df)} å³° "
                                f"(è¿‡æ»¤ç‡: {filter_stats['filter_rate']:.1f}%)"
                            )
                        else:
                            logging.warning(f"æ–‡ä»¶ {csv_file.name} ç¼ºå°‘å¿…éœ€åˆ—ï¼Œè·³è¿‡å³°è¿‡æ»¤")
                    except Exception as e:
                        logging.warning(f"æ–‡ä»¶ {csv_file.name} å³°è¿‡æ»¤å¤±è´¥: {e}")
                # === å³°åˆ†ç±»å™¨è¿‡æ»¤ç»“æŸ ===

                # æå–å³°æ•°æ®
                peaks = _extract_peaks_from_df(df)
                if peaks:
                    # ä¿å­˜peakså’Œfractionä¿¡æ¯
                    peak_data[sample_id] = {
                        'peaks': peaks,
                        'fraction': fraction,
                        'strain': strain_id
                    }
                    count += 1
                    
                    # æ‰¹å¤„ç†è¿›åº¦
                    if count % 100 == 0:
                        logging.info(f"å·²å¤„ç† {count} ä¸ªå³°æ–‡ä»¶")
            except Exception as e:
                logging.warning(f"å¤„ç†æ–‡ä»¶ {csv_file} æ—¶å‡ºé”™: {str(e)}")
        
        logging.info(f"å·²åŠ è½½ {count} ä¸ªå³°æ–‡ä»¶ï¼Œå…± {len(peak_data)} ä¸ªå³°æ•°æ®è®°å½•")
        
        # ğŸ”§ æ•°æ®è´¨é‡ç»Ÿè®¡
        fraction_counts = defaultdict(int)
        strain_counts = defaultdict(int)
        for sample_id, info in peak_data.items():
            fraction_counts[info['fraction']] += 1
            strain_counts[info['strain']] += 1
        
        logging.info(f"æ•°æ®ç»Ÿè®¡ï¼š")
        logging.info(f"  - èŒæ ªæ•°é‡: {len(strain_counts)}")
        logging.info(f"  - Fractionåˆ†å¸ƒ: {dict(fraction_counts)}")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        if len(peak_data) == 0:
            logging.warning("æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•å³°æ•°æ®ï¼Œè¿™å¯èƒ½ä¼šå½±å“åˆ†æç»“æœ")
        
        return peak_data
    except Exception as e:
        logging.error(f"å¤„ç†å³°æ•°æ®å¤±è´¥: {str(e)}")
        logging.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        if count > 0:
            logging.info(f"å·²æˆåŠŸå¤„ç† {count} ä¸ªå³°æ–‡ä»¶ï¼Œç»§ç»­ä½¿ç”¨éƒ¨åˆ†æ•°æ®")
            return peak_data
        raise


def _extract_peaks_from_df(df):
    """ä»DataFrameä¸­æå–å³°æ•°æ®
    
    å‚æ•°:
        df (pandas.DataFrame): åŒ…å«å³°æ•°æ®çš„DataFrame
        
    è¿”å›:
        list: å³°å€¼åˆ—è¡¨ï¼ˆé™åºæ’åˆ—ï¼Œä»é«˜ppmåˆ°ä½ppmï¼‰
    """
    peaks = []
    
    # å°è¯•æ‰¾åˆ°åŒ…å«ppmæˆ–X_PPMçš„åˆ—
    ppm_col = None
    for col_name in ['ppm', 'X_PPM', 'x_ppm', 'chemical_shift']:
        if col_name in df.columns:
            ppm_col = col_name
            break
    
    if ppm_col is None and 'X_AXIS' in df.columns:
        ppm_col = 'X_AXIS'
    
    if ppm_col is None and len(df.columns) > 0:
        # å°è¯•ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºppmåˆ—
        ppm_col = df.columns[0]
    
    if ppm_col is not None:
        # æå–å³°å€¼ - ğŸ”§ ç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯æ•°å€¼ç±»å‹
        for _, row in df.iterrows():
            try:
                peak_value = row[ppm_col]
                # ğŸ”§ å¼ºåˆ¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œè·³è¿‡æ— æ•ˆå€¼
                if pd.isna(peak_value):
                    continue
                peak = float(peak_value)
                if 0 <= peak <= 15:  # åˆç†çš„ppmèŒƒå›´
                    peaks.append(peak)
            except (ValueError, TypeError) as e:
                # è®°å½•æ— æ³•è½¬æ¢çš„å€¼
                logging.debug(f"è·³è¿‡æ— æ•ˆå³°å€¼: {row[ppm_col]} (é”™è¯¯: {e})")
                continue
        
        # èšç±»å¤„ç†å³°
        if peaks:
            peaks = _cluster_peaks(peaks)
            # ç¡®ä¿å³°å€¼é™åºæ’åˆ—ï¼ˆä»é«˜ppmåˆ°ä½ppmï¼‰
            peaks.sort(reverse=True)
    
    return peaks


def _cluster_peaks(raw_peaks):
    """èšç±»å¤„ç†å³°ï¼Œå°†ç›¸è¿‘çš„å³°å€¼åˆå¹¶
    
    æ ¹æ®ç”¨æˆ·åˆ†æï¼Œå®¹å·®åŒ¹é…å¯èƒ½å·²ç»è¶³å¤Ÿï¼Œä½†ä¿ç•™æ­¤å‡½æ•°ç”¨äºï¼š
    1. å¤„ç†Deep Pickerå¯èƒ½æ‹¾å–çš„é‡å¤å³°
    2. å°†0.0001èŒƒå›´å†…çš„å¾®å°å·®å¼‚ç»Ÿä¸€
    
    å‚æ•°:
        raw_peaks (list): åŸå§‹å³°å€¼åˆ—è¡¨
        
    è¿”å›:
        list: èšç±»åçš„å³°å€¼åˆ—è¡¨
    """
    if not raw_peaks:
        return []
    
    try:
        X = np.array(raw_peaks).reshape(-1, 1)
        # ä½¿ç”¨0.001çš„epsä¸å®¹å·®åŒ¹é…ä¿æŒä¸€è‡´
        db = DBSCAN(eps=0.001, min_samples=1).fit(X)
        
        clusters = defaultdict(list)
        for i, label in enumerate(db.labels_):
            clusters[label].append(raw_peaks[i])
        
        # æ¯ä¸ªç°‡å–æœ€å°å€¼ä½œä¸ºä»£è¡¨
        return [round(min(cluster), 3) for cluster in clusters.values()]
    except Exception as e:
        logging.error(f"å³°èšç±»å¤„ç†å¤±è´¥: {str(e)}")
        # å¤±è´¥æ—¶ç›´æ¥è¿”å›åŸå§‹å³°
        return [round(p, 3) for p in raw_peaks]